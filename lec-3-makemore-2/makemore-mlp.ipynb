{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2307e1ab",
   "metadata": {},
   "source": [
    "Gemini generated In depth summary \n",
    "Based on the video chapters and your code, here is a step-by-step guide to implementing the multi-layer perceptron (MLP) language model.\n",
    "\n",
    "***\n",
    "\n",
    "### Part 1: Dataset and Model Architecture\n",
    "\n",
    "1.  **Create the Dataset**:\n",
    "    * Load the `names.txt` file and define your character vocabulary (`stoi`, `itos`).\n",
    "    * Choose a `block_size` (context length), which is the number of previous characters used to predict the next one.\n",
    "    * Iterate through each word and create a list of contexts (`X`) and their corresponding next characters (`Y`). The `.` token is used to pad the context at the beginning and signal the end of a word.\n",
    "    * Shuffle the words and split the dataset into **training (80%)**, **validation (10%)**, and **test (10%)** sets. Use `Xtr, Ytr`, `Xdev, Ydev`, and `Xte, Yte` to store these.\n",
    "2.  **Initialize the Neural Network**:\n",
    "    * **Embedding Layer**: Create an embedding lookup table `C` as a `27x10` tensor. Each row represents a character, and the 10 values are its **embedding**. This is a trainable parameter.\n",
    "    * **Hidden Layer**: Define the weights `W1` (a `30x200` tensor, `30` because `block_size * embedding_size = 3 * 10`) and biases `b1` (`200` elements).\n",
    "    * **Output Layer**: Define the weights `W2` (`200x27`) and biases `b2` (`27` elements). The output size matches the number of characters.\n",
    "    * Put all these tensors (`C, W1, b1, W2, b2`) into a list called `parameters` and set `requires_grad=True` for all of them. \n",
    "\n",
    "***\n",
    "\n",
    "### Part 2: Training and Evaluation\n",
    "\n",
    "1.  **Set Up the Training Loop**:\n",
    "    * Loop for a specified number of iterations (e.g., 200,000).\n",
    "    * For each iteration, construct a **minibatch** by randomly selecting a small number of indices (`ix`) from your training data `Xtr` and `Ytr`.\n",
    "2.  **Forward Pass**:\n",
    "    * Perform an **embedding lookup**: Use `Xtr[ix]` to get the embeddings from `C`, resulting in a tensor of shape `(batch_size, block_size, embedding_size)`.\n",
    "    * Reshape the embeddings into a single vector per example using `.view(-1, block_size * embedding_size)`.\n",
    "    * Pass this through the hidden layer: compute `emb.view(...) @ W1 + b1` and apply the **tanh activation function**.\n",
    "    * Pass the hidden layer output through the output layer: compute `h @ W2 + b2`. This gives you the `logits`.\n",
    "    * Calculate the **loss** using PyTorch's `F.cross_entropy`, passing in the `logits` and the labels `Ytr[ix]`. This function efficiently combines `softmax`, `log`, and `mean`.\n",
    "3.  **Backward Pass and Update**:\n",
    "    * Zero out the gradients for all parameters by setting `p.grad = None` for each parameter `p`.\n",
    "    * Call `loss.backward()` to compute the gradients.\n",
    "    * Update the parameters using a learning rate: `p.data += -lr * p.grad`. Use a decaying learning rate, such as starting with `0.1` and dropping to `0.01` after a certain number of steps.\n",
    "4.  **Evaluate and Visualize**:\n",
    "    * After training, evaluate the loss on the validation set (`Xdev, Ydev`) to check for **overfitting**.\n",
    "    * Visualize the embedding space by plotting the first two dimensions of the `C` matrix. Each point represents a character.\n",
    "\n",
    "***\n",
    "\n",
    "### Part 3: Sampling and Conclusion\n",
    "\n",
    "1.  **Sample from the Model**:\n",
    "    * Start with an initial `context` of all `.` tokens.\n",
    "    * Enter a loop that continues until the model predicts a `.` token.\n",
    "    * Inside the loop, get the embeddings for the current `context` from the trained `C` matrix.\n",
    "    * Perform a forward pass through the hidden and output layers to get the `logits`.\n",
    "    * Apply `F.softmax` to the logits to get probabilities.\n",
    "    * Use `torch.multinomial` to sample the index of the next character.\n",
    "    * Append the new index to your output list and update the `context` by sliding the window.\n",
    "    * Finally, join the characters from the output list to form a new name.\n",
    "\n",
    "    ---\n",
    "    ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b893817",
   "metadata": {},
   "source": [
    "GPT summary walkthrough , with lesser help , covering all ideas in the code \n",
    "---\n",
    "# üß† Character-Level MLP Language Model ‚Äî Complete From-Scratch Walkthrough\n",
    "\n",
    "This document summarizes the **entire lecture** so you can reimplement the model without looking at the original code.  \n",
    "It covers **every step**: data prep, architecture, training, and sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Problem Setup\n",
    "\n",
    "We want to train a **character-level language model** that generates new names.  \n",
    "The model will be an **MLP** (multi-layer perceptron) trained from scratch on a dataset of names.\n",
    "\n",
    "The model‚Äôs job:  \n",
    "Given a **context** (a fixed number of previous characters), predict the **next character**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Data Preparation\n",
    "\n",
    "1. **Load Dataset**  \n",
    "   - Read the `names.txt` file into a list of strings, one name per line.  \n",
    "   - Inspect dataset: size, min/max length.\n",
    "\n",
    "2. **Define Vocabulary**  \n",
    "   - Collect all unique characters in the dataset.  \n",
    "   - Add a special `.` token for start/end of a word.  \n",
    "   - Create two dictionaries:\n",
    "     - `stoi`: char ‚Üí index\n",
    "     - `itos`: index ‚Üí char\n",
    "\n",
    "3. **Context Windows**  \n",
    "   - Choose a fixed context size `block_size` (e.g., 3).  \n",
    "   - For each name:\n",
    "     - Pad with `.` tokens at the start.\n",
    "     - Slide a window of length `block_size` across the name.\n",
    "     - The window characters are the **input**.\n",
    "     - The next character is the **target**.\n",
    "\n",
    "4. **Numerical Encoding**  \n",
    "   - Map characters in the context and the target to integers using `stoi`.  \n",
    "   - Store all contexts in an integer tensor `X`.  \n",
    "   - Store all targets in integer tensor `Y`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Model Architecture\n",
    "\n",
    "The MLP has three main parts:\n",
    "\n",
    "1. **Embedding Layer**  \n",
    "   - A learnable matrix `C` of size `(vocab_size, embedding_dim)`.  \n",
    "   - Converts each character index into a dense vector.\n",
    "\n",
    "2. **Hidden Layer**  \n",
    "   - Flatten all embeddings for the context into a single vector.  \n",
    "   - Apply a linear transformation: `h = tanh(X @ W1 + b1)`  \n",
    "     - `W1`: weight matrix of shape `(context_size * embedding_dim, hidden_size)`\n",
    "     - `b1`: bias vector of length `hidden_size`.\n",
    "\n",
    "3. **Output Layer**  \n",
    "   - Map hidden activations to vocabulary logits: `logits = h @ W2 + b2`  \n",
    "     - `W2`: weight matrix `(hidden_size, vocab_size)`\n",
    "     - `b2`: bias vector `(vocab_size,)`\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Loss Function\n",
    "\n",
    "We use **cross-entropy loss** between predicted logits and target indices.\n",
    "\n",
    "Two ways to compute:\n",
    "1. **Manual**: softmax ‚Üí log ‚Üí negative log likelihood ‚Üí mean over batch.\n",
    "2. **Built-in**: `torch.nn.functional.cross_entropy(logits, targets)`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Training Loop\n",
    "\n",
    "1. **Initialization**  \n",
    "   - Randomly initialize all weights with small values (e.g., normal distribution).  \n",
    "   - Zero biases.\n",
    "\n",
    "2. **Forward Pass**  \n",
    "   - Embed context characters ‚Üí concatenate ‚Üí hidden layer ‚Üí output layer.  \n",
    "   - Compute loss vs targets.\n",
    "\n",
    "3. **Backward Pass**  \n",
    "   - Call `.backward()` on loss to compute gradients.\n",
    "\n",
    "4. **Parameter Update**  \n",
    "   - Update all parameters with gradient descent:  \n",
    "     `param -= learning_rate * param.grad`  \n",
    "   - Zero gradients after each update.\n",
    "\n",
    "5. **Minibatch Training**  \n",
    "   - Shuffle dataset each epoch.  \n",
    "   - Train in batches for efficiency.\n",
    "\n",
    "6. **Learning Rate Tuning**  \n",
    "   - Try a small range of learning rates.  \n",
    "   - Pick one that leads to fastest stable loss decrease.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Train/Validation/Test Split\n",
    "\n",
    "- Split dataset: 80% train, 10% val, 10% test.  \n",
    "- Train only on training set, tune hyperparameters on val set, report final test loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Experiments & Insights\n",
    "\n",
    "- **Bigger Hidden Layer**: more capacity, better fit.  \n",
    "- **Bigger Embedding Dim**: richer character representations.  \n",
    "- **Regularization**: optional L2 penalty to reduce overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Sampling from the Model\n",
    "\n",
    "To generate a name:\n",
    "1. Start with `.` tokens as context.\n",
    "2. Predict probability distribution over next char.\n",
    "3. Sample a char from distribution.\n",
    "4. Shift context, append new char.\n",
    "5. Repeat until `.` is generated (end of name).\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Visualizing Embeddings\n",
    "\n",
    "- After training, the embedding matrix `C` contains a vector for each character.  \n",
    "- You can plot them in 2D (e.g., PCA or t-SNE) to see relationships between characters.\n",
    "\n",
    "---\n",
    "\n",
    "## üîü Full Process Recap\n",
    "\n",
    "1. Load data & build vocab.  \n",
    "2. Create context‚Äìtarget pairs.  \n",
    "3. Encode to integers.  \n",
    "4. Build embedding + MLP layers.  \n",
    "5. Train with cross-entropy loss.  \n",
    "6. Tune hyperparameters.  \n",
    "7. Generate samples.  \n",
    "8. Visualize learned embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "**End Goal**: A fully trained MLP that can generate realistic-looking new names purely from character-level probabilities learned on the training set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369cf9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# let's code \n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee597aef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
