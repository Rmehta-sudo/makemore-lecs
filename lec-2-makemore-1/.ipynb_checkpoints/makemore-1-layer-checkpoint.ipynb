{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3020df26",
   "metadata": {},
   "source": [
    "So we are starting with the bigram model first\n",
    "\n",
    "Using Gemini to collect all the topics in one place \n",
    "To implement the bigram models from Andrej Karpathy's lectures, you can follow these steps. This outline combines the key concepts from the video with the code you've provided, creating a clear implementation path.\n",
    "\n",
    "***\n",
    "\n",
    "### Part 1: Statistical Bigram Model\n",
    "\n",
    "1.  **Prepare the Data**\n",
    "    * Load the `names.txt` dataset and split it into a list of words.\n",
    "    * Create a vocabulary of all unique characters in the dataset.\n",
    "    * Establish a mapping from characters to integers (`stoi`) and integers to characters (`itos`), including a special `.` token for start and end characters. \n",
    "2.  **Count Bigrams**\n",
    "    * Initialize a 27x27 PyTorch tensor, `N`, filled with zeros. This will serve as your bigram count table.\n",
    "    * Iterate through each word in the dataset.\n",
    "    * For each word, create a list of bigrams by prepending and appending the `.` token.\n",
    "    * For each bigram, use the `stoi` mapping to get the integer indices for the two characters and increment the corresponding cell in the `N` tensor.\n",
    "3.  **Analyze and Sample from the Model**\n",
    "    * Visualize the `N` tensor using `matplotlib` to see the bigram frequencies.\n",
    "    * Normalize the bigram counts to get probabilities. Create a new tensor `P` by converting `N` to a float, and then dividing each row by its sum. Use `N+1` before normalizing to implement **model smoothing**.\n",
    "    * To generate new names, start with the `.` token. Use `torch.multinomial` on the probability distribution for the current character to sample the next character.\n",
    "    * Repeat this process, appending the sampled character to an output list until a `.` is sampled.\n",
    "4.  **Evaluate the Model**\n",
    "    * Calculate the **negative log-likelihood (NLL)**.\n",
    "    * Iterate through all bigrams in your training data.\n",
    "    * For each bigram, get the probability from your `P` tensor.\n",
    "    * Compute the log of this probability (`torch.log(prob)`).\n",
    "    * Sum up all the log probabilities and then negate the sum to get the total log-likelihood.\n",
    "    * The average NLL (total NLL divided by the number of bigrams) is your loss.\n",
    "\n",
    "***\n",
    "\n",
    "### Part 2: Neural Network Bigram Model\n",
    "\n",
    "1.  **Prepare Neural Network Data**\n",
    "    * Create a dataset of `(x, y)` pairs, where `x` is the index of the first character of a bigram and `y` is the index of the second.\n",
    "    * Store these pairs in PyTorch tensors `xs` and `ys`.\n",
    "2.  **Define and Train the Neural Network**\n",
    "    * Initialize a `27x27` weight matrix `W` with random values. This matrix represents the \"neural network.\" Set `requires_grad=True` to enable backpropagation.\n",
    "    * Implement the training loop:\n",
    "        * **Forward Pass**:\n",
    "            * Convert the input `xs` into a one-hot encoded tensor `xenc` of shape `(num_examples, 27)`.\n",
    "            * Compute the \"logits\" by performing a matrix multiplication: `logits = xenc @ W`.\n",
    "            * Apply the softmax function to the logits to get probabilities: `counts = logits.exp()` and `probs = counts / counts.sum(1, keepdims=True)`.\n",
    "            * Calculate the **loss**: Use the negative log-likelihood of the probabilities for the correct characters (`ys`). The code `loss = -probs[torch.arange(num), ys].log().mean()` is a vectorized way to do this. Add a regularization term like `0.01*(W**2).mean()` to prevent the model from becoming too confident (similar to model smoothing).\n",
    "        * **Backward Pass**:\n",
    "            * Call `loss.backward()` to compute the gradients of the loss with respect to the weight matrix `W`.\n",
    "        * **Update Weights**:\n",
    "            * Update the weights using a small learning rate multiplied by the gradients: `W.data += -learning_rate * W.grad`.\n",
    "            * Repeat this loop for multiple epochs to train the model.\n",
    "3.  **Sample from the Neural Network**\n",
    "    * Start with the `.` token (index 0).\n",
    "    * Create a one-hot encoded tensor for the current character's index.\n",
    "    * Pass this tensor through the trained neural network (`xenc @ W`) and apply the softmax to get the probability distribution for the next character.\n",
    "    * Use `torch.multinomial` to sample the next character's index from this distribution.\n",
    "    * Repeat the process until the `.` token is sampled, then join the characters to form a name.\n",
    "\n",
    "    ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4242318",
   "metadata": {},
   "source": [
    "*** \n",
    "This is a GPT generated - slightly less assisted to-do list , will refer to the above when stuck and to the below for my main stuff\n",
    "***\n",
    "# Bigram Language Model & Neural Net Implementation — Step-by-Step Plan\n",
    "\n",
    "This notebook will implement a bigram character model and then extend it to a small neural network, **from scratch**.  \n",
    "The following steps are the roadmap to follow.\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 1 — Bigram Model (Statistical Counting)**\n",
    "\n",
    "1. **Read and Inspect the Dataset**\n",
    "   - Load `names.txt` into a Python list of strings.\n",
    "   - Print sample names, dataset size, min/max name length.\n",
    "\n",
    "2. **Count Bigrams with a Python Dictionary**\n",
    "   - Add start (`.`) and end (`.`) tokens around each word.\n",
    "   - Count `(ch1, ch2)` pairs using a dictionary.\n",
    "\n",
    "3. **Count Bigrams with a Torch Tensor**\n",
    "   - Create mappings:\n",
    "     - `stoi` — string to index\n",
    "     - `itos` — index to string\n",
    "   - Build a `27×27` count matrix `N` using tensor indexing.\n",
    "\n",
    "4. **Visualize the Bigram Matrix**\n",
    "   - Plot counts as an image with `matplotlib`.\n",
    "   - Overlay `(ch1, ch2)` pairs and their counts.\n",
    "\n",
    "5. **Normalize to Probabilities**\n",
    "   - Convert counts to probabilities `P` by row-wise normalization.\n",
    "   - Sample characters using `torch.multinomial`.\n",
    "\n",
    "6. **Sampling from the Bigram Model**\n",
    "   - Generate random names by repeatedly sampling until the end token.\n",
    "\n",
    "7. **Loss Function (Negative Log Likelihood)**\n",
    "   - Compute average negative log likelihood (NLL) of the dataset.\n",
    "   - Understand relationship between NLL, log-likelihood, and cross-entropy.\n",
    "\n",
    "8. **Model Smoothing**\n",
    "   - Apply add-one (Laplace) smoothing to avoid zero probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 2 — Neural Network Bigram Model**\n",
    "\n",
    "9. **Create Training Dataset**\n",
    "   - For each bigram: store input index (`xs`) and target index (`ys`).\n",
    "\n",
    "10. **One-Hot Encoding**\n",
    "    - Convert `xs` into one-hot vectors of size 27.\n",
    "    - Inspect shapes and visualize.\n",
    "\n",
    "11. **Initialize Linear Layer Weights**\n",
    "    - Randomly initialize `W` with shape `(27, 27)`.\n",
    "\n",
    "12. **Forward Pass**\n",
    "    - Compute logits: `xenc @ W`.\n",
    "    - Apply softmax to get probabilities.\n",
    "\n",
    "13. **Loss Computation**\n",
    "    - Extract predicted probability for each correct target in `ys`.\n",
    "    - Compute mean NLL loss.\n",
    "\n",
    "14. **Vectorized Implementation**\n",
    "    - Perform the loss computation for the entire batch without loops.\n",
    "\n",
    "15. **Backward Pass and Update**\n",
    "    - Zero gradients, call `.backward()`, update `W` with gradient descent.\n",
    "\n",
    "16. **Regularization**\n",
    "    - Add L2 penalty term (`0.01*(W**2).mean()`) to the loss.\n",
    "\n",
    "17. **Train the Model**\n",
    "    - Repeat forward–backward–update steps until loss converges.\n",
    "\n",
    "18. **Sampling from the Neural Net**\n",
    "    - Use the trained neural net to sample names.\n",
    "    - Replace bigram table sampling with network predictions.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ecbc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc75ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# let's code \n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdea7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
